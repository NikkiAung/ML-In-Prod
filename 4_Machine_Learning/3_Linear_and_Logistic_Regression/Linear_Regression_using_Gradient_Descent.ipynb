{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "From what I understand, Linear Regression is based on two things dependent variable and independent variable (one or more), it is just basically try to sit in the best middle fit to, using Math equation y = mx + c, and manipulating m c in order to get the best fit line on the graph. \n",
    "\n",
    "For simple linear regression (one feature), you can directly calculate the best fit line using closed-form equations (e.g., the normal equation), using least squares method. \n",
    "\n",
    "Note that this is for **small datasets or simple problems.**\n",
    "\n",
    "For large datasets or high-dimensional problems, we use **Gradient Descent** is an optimization algorithm used to minimize the cost function (e.g., MSE).\n",
    "\n",
    "Instead of calculating the optimal solution directly (which can be computationally expensive for large datasets), Gradient Descent iteratively adjusts the parameters of the line (slope and intercept) to minimize the error.\n",
    "1. It starts with random parameter values. **y-intercept c and slope**\n",
    "\n",
    "2. It calculates the gradient of the cost function with respect to the parameters.\n",
    "\n",
    "3. It updates the parameters in the direction of the negative gradient to reduce the error.\n",
    "\n",
    "4. This process repeats until the cost function reaches its minimum (the lowest error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
